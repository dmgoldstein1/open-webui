# Open WebUI - Copilot Instructions

<!-- Generated by GitHub Copilot on November 2, 2025 -->
<!-- This file provides essential guidance for AI coding agents working on the Open WebUI codebase -->

## Project Overview

Open WebUI is a self-hosted AI platform with a **dual-architecture design**:
- **Frontend**: SvelteKit SPA (`src/`) - Static build serving the web interface
- **Backend**: FastAPI server (`backend/open_webui/`) - API server handling LLM integrations
- **LLM Gateway**: Optional serverless routing layer (`llm-gateway/`) - AWS-style routing between vLLM/Ollama

## Critical Architecture Patterns

### 1. API Router Structure
Backend follows a **modular router pattern** in `backend/open_webui/routers/`:
- Each service has dedicated router (e.g., `openai.py`, `ollama.py`, `chats.py`)
- Routers are mounted in `main.py` with `/api/v1` prefix
- Authentication handled via `get_verified_user()` dependency

### 2. Frontend-Backend Communication
```typescript
// Frontend API calls use structured client pattern
// All endpoints defined in src/lib/apis/{service}/index.ts
import { getModels } from '$lib/apis/models';
```

### 3. LLM Provider Integration
The system supports **multiple LLM backends simultaneously**:
- **Ollama**: Local models via `/ollama` proxy endpoints
- **OpenAI-compatible**: External APIs via `/openai` proxy
- **Built-in inference**: For RAG and embeddings

## Essential Development Workflows

### Local Development Setup
```bash
# Backend (required)
cd backend && pip install -r requirements.txt
./start.sh  # Runs on :8080

# Frontend (required for UI changes)
npm install && npm run dev  # Runs on :5173, proxies to backend

# Docker (alternative)
make install  # Uses docker-compose.yaml
```

### Key Environment Variables
- `OLLAMA_BASE_URL`: Points to Ollama instance (default: `http://localhost:11434`)
- `WEBUI_SECRET_KEY`: JWT signing (auto-generated if missing)
- `DATA_DIR`: SQLite/file storage location

### Testing Strategy
- **Frontend**: `npm run test:frontend` (Vitest)
- **E2E**: `npm run cy:open` (Cypress in `cypress/e2e/`)
- **Backend**: Manual testing via FastAPI docs at `/docs`
- **LLM Gateway**: `npm test` in `llm-gateway/` directory

## Project-Specific Conventions

### File Organization
- **Models**: SQLAlchemy ORM in `backend/open_webui/models/`
- **API Clients**: TypeScript interfaces in `src/lib/apis/{service}/`
- **UI Components**: Svelte components in `src/lib/components/`
- **State Management**: Svelte stores in `src/lib/stores/`

### Database Patterns
Uses **SQLite with Alembic migrations**:
```python
# Migration location: backend/open_webui/migrations/
# Run migrations: alembic upgrade head
```

### Authentication Flow
1. Users authenticate via `/api/v1/auths` endpoints
2. JWT tokens stored in browser localStorage
3. Backend validates via `get_verified_user()` dependency
4. Role-based access via `UserModel.get_user_by_token()`

## Integration Points

### LLM Communication
- **Unified interface**: All LLM calls use OpenAI-compatible `/v1/chat/completions`
- **Streaming**: WebSocket connections for real-time responses
- **Context management**: Conversation history stored in `chats` table

### RAG Pipeline
- **Document ingestion**: Via `/api/v1/retrieval` endpoints
- **Embedding generation**: Handled by backend inference engine
- **Vector storage**: Configurable backends (ChromaDB, etc.)

### Plugin System (Pipelines)
- **External functions**: Python functions via `pipelines` framework
- **Integration**: Set OpenAI URL to Pipelines instance
- **Examples**: Rate limiting, translation, content filtering

## Common Development Tasks

### Adding New LLM Provider
1. Create router in `backend/open_webui/routers/{provider}.py`
2. Add API client in `src/lib/apis/{provider}/`
3. Update model selection UI in `src/lib/components/`
4. Test integration in both streaming and batch modes

### Modifying UI Components
- Components are **Svelte 4** with TypeScript
- Use **Tailwind CSS** for styling (see `tailwind.config.js`)
- State via stores, not props drilling
- All text should be **i18n-ready** (see `src/lib/i18n/`)

### Docker Deployment
- **Multi-stage build**: Frontend assets copied to backend container
- **Volume persistence**: `/app/backend/data` for database/files
- **Network configuration**: Use `--network=host` for local Ollama access

## Critical Files to Understand

- `backend/open_webui/main.py`: FastAPI app setup and router mounting
- `src/lib/constants.ts`: Frontend configuration and API endpoints  
- `docker-compose.yaml`: Standard deployment configuration
- `svelte.config.js`: Build configuration for static SPA
- `backend/requirements.txt`: Python dependencies and versions

## Debugging Patterns

### Backend Issues
- Check logs via `uvicorn` output or container logs
- Use `/docs` endpoint for API exploration
- Database issues: Check `backend/open_webui/data/webui.db`

### Frontend Issues
- Browser dev tools for API calls
- Vite dev server shows build errors
- Check console for WebSocket connection failures

### LLM Integration Issues
- Verify provider URLs in admin settings
- Check model availability via `/api/v1/models`
- Test direct provider APIs outside Open WebUI first