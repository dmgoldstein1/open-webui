# AWS/Moto configuration
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test
AWS_SESSION_TOKEN=test

# Moto endpoints (set to your moto server URLs; leave empty for real AWS)
DYNAMODB_ENDPOINT_URL=http://localhost:5000
SQS_ENDPOINT_URL=http://localhost:5000

# DynamoDB table
DDB_TABLE=Conversations

# SQS queues
SQS_QUEUE_URL=http://localhost:5000/000000000000/llm-jobs
SQS_PRIORITY_QUEUE_URL=http://localhost:5000/000000000000/llm-jobs-priority

# Local LLM endpoints
# For both workers we use OpenAI-compatible chat API for simplicity
VLLM_BASE_URL=http://localhost:8000
VLLM_MODEL=llama-3-8b-instruct

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_OPENAI_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llava:13b

# Server
PORT=8787
ALLOW_ORIGIN=*
