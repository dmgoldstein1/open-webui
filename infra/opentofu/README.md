# OpenTofu IaC for LLM Gateway (Local Dev)

Generated by Copilot (OpenAI)

This configuration provisions local infrastructure for the gateway/queue project:

 - Optional: Create an API Gateway HTTP API in LocalStack that proxies all routes to your local Express server (set `-var enable_apigw=true`). This uses `local_api_base_url` (defaults to `http://host.docker.internal:8787`).

## Usage

```bash
cd infra/opentofu
# Initialize and apply
tofu init
tofu apply -auto-approve


To enable LocalStack API Gateway proxying to your local Express server, pass:

```bash
tofu apply -auto-approve -var enable_apigw=true -var local_api_base_url=http://host.docker.internal:8787
```
Note: On some Linux setups, `host.docker.internal` may not resolve for containers; substitute your host IP if needed.
# Export env vars for llm-gateway
export AWS_REGION=us-east-1
export AWS_ACCESS_KEY_ID=test
export AWS_SECRET_ACCESS_KEY=test
export SQS_ENDPOINT_URL=http://localhost:4566
export DYNAMODB_ENDPOINT_URL=http://localhost:8000
export SQS_QUEUE_URL=$(tofu output -raw sqs_default_queue_url)
export SQS_PRIORITY_QUEUE_URL=$(tofu output -raw sqs_priority_queue_url)
export DDB_TABLE=Conversations

# Run local API and worker
npm --prefix ../../llm-gateway install
node ../../llm-gateway/src/local/server.js &
node ../../llm-gateway/src/worker/worker.js
```

## Deploying to AWS (example)

The `main.tf` includes commented-out resources showing how to switch to real AWS. Uncomment the provider and resources, set proper credentials, and apply in a non-local environment.
